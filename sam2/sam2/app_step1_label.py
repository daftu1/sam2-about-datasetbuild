import streamlit as st
import cv2
import os
import numpy as np
from PIL import Image
from ultralytics import SAM
import torch
import uuid

# åˆå§‹åŒ–SAM2æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡åŠ è½½å¯èƒ½è¾ƒæ…¢ï¼‰
@st.cache_resource
def load_sam_model():
    return SAM("sam2.1_b.pt")  # æˆ–ä½ è‡ªå·±çš„å¾®è°ƒsamæ¨¡å‹è·¯å¾„

sam_model = load_sam_model()

# è®¾ç½®ç¼“å­˜ç›®å½•
FRAME_DIR = "sample_frames"
MASK_DIR = "saved_masks"
os.makedirs(FRAME_DIR, exist_ok=True)
os.makedirs(MASK_DIR, exist_ok=True)

st.title("ğŸ¯ Step 1ï¼šé‡‡æ ·å¸§ + SAM2 æ©è†œæ‰“ç‚¹æ ‡æ³¨")

uploaded_video = st.file_uploader("ä¸Šä¼ è§†é¢‘", type=["mp4"])
sampling_interval = st.number_input("æ¯éš”Nå¸§æŠ½å–1å¸§ï¼ˆå»ºè®®ä¸º10~30ï¼‰", value=15, min_value=1, max_value=100)

if uploaded_video:
    video_path = f"temp_video.mp4"
    with open(video_path, "wb") as f:
        f.write(uploaded_video.read())

    # æŠ½å¸§
    st.info("æ­£åœ¨æŠ½å–å¸§...")
    cap = cv2.VideoCapture(video_path)
    frame_idx, saved_idx = 0, 0
    frames = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx % sampling_interval == 0:
            img_path = os.path.join(FRAME_DIR, f"frame_{saved_idx:04d}.jpg")
            cv2.imwrite(img_path, frame)
            frames.append(img_path)
            saved_idx += 1
        frame_idx += 1

    cap.release()
    st.success(f"å…±æå– {len(frames)} å¸§ï¼Œå‡†å¤‡è¿›å…¥æ ‡æ³¨æµç¨‹ã€‚")

    frame_idx = st.slider("é€‰æ‹©å¸§ç¼–å·", 0, len(frames) - 1)
    selected_frame = cv2.imread(frames[frame_idx])
    rgb_img = cv2.cvtColor(selected_frame, cv2.COLOR_BGR2RGB)
    st.image(rgb_img, caption=f"Frame {frame_idx}", channels="RGB")

    st.subheader("ğŸ“Œ æ©è†œæ‰“ç‚¹")
    x_click = st.number_input("ç‚¹å‡»ç‚¹ X åæ ‡", min_value=0, max_value=selected_frame.shape[1] - 1)
    y_click = st.number_input("ç‚¹å‡»ç‚¹ Y åæ ‡", min_value=0, max_value=selected_frame.shape[0] - 1)
    label = st.text_input("è¯·è¾“å…¥è¯¥æ©è†œçš„ç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚ï¼šé”…ã€è¾£æ¤’ï¼‰")

    if st.button("ç”Ÿæˆæ©è†œ"):
        if label.strip() == "":
            st.warning("è¯·å¡«å†™ç±»åˆ«æ ‡ç­¾")
        else:
            prompts = {"points": [[x_click, y_click]], "labels": [1]}  # æ­£æ ·æœ¬ç‚¹
            result = sam_model(selected_frame, **prompts)[0]
            mask = result.masks.data[0].cpu().numpy().astype(np.uint8) * 255

            # å¯è§†åŒ–
            color_mask = cv2.applyColorMap(mask, cv2.COLORMAP_JET)
            combined = cv2.addWeighted(rgb_img, 0.6, color_mask, 0.4, 0)
            st.image(combined, caption=f"æ©è†œé¢„è§ˆï¼š{label}", channels="RGB")

            # ä¿å­˜
            uid = uuid.uuid4().hex[:8]
            cv2.imwrite(os.path.join(MASK_DIR, f"{label}_{frame_idx}_{uid}.png"), mask)
            with open(os.path.join(MASK_DIR, f"{label}_{frame_idx}_{uid}.txt"), "w") as f:
                f.write(f"{label},{x_click},{y_click},{frames[frame_idx]}")
            st.success("å·²ä¿å­˜è¯¥æ©è†œä¸æ ‡æ³¨ä¿¡æ¯ï¼")
