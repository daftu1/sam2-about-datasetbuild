
import streamlit as st
st.set_page_config(layout="wide")  # å¿…é¡»ä¸ºé¦–è¡Œ

import os, uuid, cv2, shutil, numpy as np, torch
from PIL import Image
from moviepy import VideoFileClip
from ultralytics import SAM
from streamlit_image_coordinates import streamlit_image_coordinates
from sam2.build_sam import build_sam2_video_predictor
from torchvision.ops import masks_to_boxes

device = "cuda" if torch.cuda.is_available() else "cpu"

sam2_checkpoint = "../checkpoints/sam2.1_hiera_large.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"


predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)

inference_state = None

st.title("ğŸ¬ Step 2ï¼šè§†é¢‘æ‰“ç‚¹ & å¤šå¸§æ©ç ç”Ÿæˆ")

VIDEO_DIR = "video_segments"
os.makedirs(VIDEO_DIR, exist_ok=True)

# ä¸Šä¼ åŸå§‹è§†é¢‘
uploaded_video = st.file_uploader("ğŸ“ ä¸Šä¼ å®Œæ•´è§†é¢‘", type=["mp4"])
if uploaded_video:
    original_path = "temp_uploaded.mp4"
    with open(original_path, "wb") as f:
        f.write(uploaded_video.read())

    cap = cv2.VideoCapture(original_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    duration = frame_count // fps
    st.video(original_path)

    start_time = st.slider("èµ·å§‹æ—¶é—´ (ç§’)", 0, duration - 1, 0)
    end_time = st.slider("ç»“æŸæ—¶é—´ (ç§’)", start_time + 1, duration, start_time + 5)

    if st.button("âœ‚ï¸ è£å‰ªè§†é¢‘æ®µ"):
        clip = VideoFileClip(original_path).subclipped(start_time, end_time)
        session_id = uuid.uuid4().hex[:8]
        segment_path = os.path.join(VIDEO_DIR, f"{session_id}.mp4")
        clip.write_videofile(segment_path, codec="libx264")
        st.success(f"âœ… è£å‰ªå®Œæˆ: {segment_path}")
        st.session_state["segment_path"] = segment_path
        st.session_state["session_id"] = session_id
        shutil.move(original_path, f"{original_path}.bak")

# è§†é¢‘æ©ç æ ‡æ³¨
if "segment_path" in st.session_state:
    segment_path = st.session_state["segment_path"]
    cap = cv2.VideoCapture(segment_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    st.video(segment_path)

    st.markdown("## ğŸ§  æ‰“ç‚¹ä¸æ©ç ä¼ æ’­")

    label = st.text_input("ç›®æ ‡æ ‡ç­¾å", value="pot")
    frame_idx = st.slider("é€‰æ‹©æ ‡æ³¨å¸§", 0, total_frames - 1, 0)

    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
    ret, selected_frame = cap.read()
    if not ret:
        st.error("æ— æ³•è¯»å–å¸§")
        st.stop()

    rgb_frame = cv2.cvtColor(selected_frame, cv2.COLOR_BGR2RGB)

    if "click_points" not in st.session_state:
        st.session_state.click_points = []
        st.session_state.click_labels = []
        st.session_state.ann_obj_id = 1

    mode = st.radio("ç‚¹å‡»æ¨¡å¼", ["æ·»åŠ ç‚¹ï¼ˆæ­£ï¼‰", "å»é™¤ç‚¹ï¼ˆè´Ÿï¼‰"])
    is_positive = 1 if "æ­£" in mode else 0

    def draw_points(image, points, labels):
        vis = image.copy()
        for (x, y), label in zip(points, labels):
            color = (0, 255, 0) if label == 1 else (255, 0, 0)
            cv2.circle(vis, (x, y), 6, color, -1)
        return vis

    annotated = draw_points(rgb_frame, st.session_state.click_points, st.session_state.click_labels)
    result = streamlit_image_coordinates(Image.fromarray(annotated), key=f"click_{frame_idx}")

    if result:
        st.session_state.click_points.append((result["x"], result["y"]))
        st.session_state.click_labels.append(is_positive)
        st.rerun()


    col1, col2 = st.columns(2)
    if col1.button("ğŸ§¹ æ¸…ç©ºç‚¹å‡»ç‚¹"):
        st.session_state.click_points = []
        st.session_state.click_labels = []
        st.experimental_rerun()

    if col2.button("ğŸš€ è¿è¡Œæ©ç ä¼ æ’­"):
        if not st.session_state.click_points:
            st.warning("è¯·å…ˆæ‰“ç‚¹")
        else:
            points = np.array(st.session_state.click_points, dtype=np.float32)
            labels_arr = np.array(st.session_state.click_labels, dtype=np.int32)
            ann_obj_id = st.session_state.ann_obj_id

            if inference_state is None:
                inference_state = predictor.get_inference_state(video_path=segment_path)

            _, out_obj_ids, _ = predictor.add_new_points_or_box(
                inference_state=inference_state,
                frame_idx=frame_idx,
                obj_id=ann_obj_id,
                points=points,
                labels=labels_arr,
            )

            video_segments = {}
            for idx, out_ids, out_logits in predictor.propagate_in_video(inference_state):
                video_segments[idx] = {
                    out_ids[i]: (out_logits[i] > 0).cpu().numpy()
                    for i in range(len(out_ids))
                }

            st.success("âœ… æ©ç ä¼ æ’­å®Œæˆ")

            export_dir = f"mask_propagation_output/{st.session_state['session_id']}_{label}"
            os.makedirs(export_dir, exist_ok=True)
            cap = cv2.VideoCapture(segment_path)
            for idx in sorted(video_segments.keys()):
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, f = cap.read()
                if not ret:
                    continue
                cv2.imwrite(f"{export_dir}/img_{idx:04d}.jpg", f)

                for obj_id, mask in video_segments[idx].items():
                    mask_path = f"{export_dir}/mask_{idx:04d}_{obj_id}.png"
                    cv2.imwrite(mask_path, mask * 255)

                    boxes = masks_to_boxes(torch.tensor(mask[None])).numpy()
                    with open(f"{export_dir}/img_{idx:04d}.txt", "w") as ftxt:
                        for box in boxes:
                            cx = (box[0] + box[2]) / 2 / mask.shape[1]
                            cy = (box[1] + box[3]) / 2 / mask.shape[0]
                            w = (box[2] - box[0]) / mask.shape[1]
                            h = (box[3] - box[1]) / mask.shape[0]
                            ftxt.write(f"0 {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\n")

            st.success(f"æ‰€æœ‰æ©ç ä¸æ ‡ç­¾å·²å¯¼å‡ºè‡³ï¼š{export_dir}")
