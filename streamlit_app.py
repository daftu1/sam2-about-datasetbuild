import os
os.environ["TORCHINDUCTOR_DISABLE"] = "1"
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import streamlit as st
st.set_page_config(layout="wide")

import os, uuid, cv2, shutil, numpy as np, torch
from PIL import Image
from moviepy import VideoFileClip
from streamlit_image_coordinates import streamlit_image_coordinates
from torchvision.ops import masks_to_boxes
from sam2.build_sam import build_sam2_video_predictor
import contextlib

@st.cache_resource
def load_sam2_model():
    checkpoint = os.path.join(os.path.expanduser("~"), "sam2", "checkpoints", "sam2.1_hiera_base_plus.pt")
    model_cfg = os.path.join("configs", "sam2.1", "sam2.1_hiera_b+.yaml")
    # è‹¥å¯ç”¨åˆ™ä½¿ç”¨GPUï¼Œå¦åˆ™é€€å›CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return build_sam2_video_predictor(model_cfg, checkpoint, device=device, vos_optimized=True)

sam2_model = load_sam2_model()

st.title("ğŸ¬ Preï¼šäº¤äº’å¼è§†é¢‘æ ‡æ³¨ & è‡ªåŠ¨æ©ç ä¼ æ’­ï¼ˆSAM2é›†æˆï¼‰")

VIDEO_DIR = "video_segments"
os.makedirs(VIDEO_DIR, exist_ok=True)

# ä¸Šä¼ è§†é¢‘
uploaded_video = st.file_uploader("ğŸ“ ä¸Šä¼ å®Œæ•´åŸå§‹è§†é¢‘", type=["mp4"])
if uploaded_video:
    original_path = "temp_uploaded.mp4"
    with open(original_path, "wb") as f:
        f.write(uploaded_video.read())

    cap = cv2.VideoCapture(original_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    duration = frame_count // fps
    st.video(original_path)

    st.subheader("âœ‚ï¸ è§†é¢‘è£å‰ª")
    start_time = st.slider("èµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰", 0, duration - 1, 0)
    end_time = st.slider("ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰", start_time + 1, duration, start_time + 5)

    if st.button("è£å‰ªå¹¶ä¿å­˜ç‰‡æ®µ"):
        clip = VideoFileClip(original_path).subclipped(start_time, end_time)
        session_id = uuid.uuid4().hex[:8]
        segment_path = os.path.join(VIDEO_DIR, f"{session_id}.mp4")
        clip.write_videofile(segment_path, codec="libx264")
        st.success(f"âœ… è§†é¢‘è£å‰ªå®Œæˆ: {segment_path}")
        st.session_state["segment_path"] = segment_path
        st.session_state["session_id"] = session_id
        cap.release()
        shutil.move(original_path, f"{original_path}.bak")

# è§†é¢‘æ‹†å¸§ + åŠ è½½çŠ¶æ€
session_id = st.session_state.get("session_id", None)
segment_path = st.session_state.get("segment_path", None)

if session_id and segment_path:
    FRAME_DIR = f"frame_cache_{session_id}"
    os.makedirs(FRAME_DIR, exist_ok=True)

    if not os.listdir(FRAME_DIR):
        cap = cv2.VideoCapture(segment_path)
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_path = os.path.join(FRAME_DIR, f"{frame_idx:04d}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_idx += 1
        cap.release()
        st.success(f"âœ… å…±æå– {frame_idx} å¸§è‡³ç¼“å­˜ç›®å½• {FRAME_DIR}")

    frame_files = sorted(os.listdir(FRAME_DIR))
    frame_index = st.session_state.get("frame_index", 0)
    current_frame_path = os.path.join(FRAME_DIR, frame_files[frame_index])
    current_img = Image.open(current_frame_path)

    frame_np = np.array(current_img.convert("RGB"))
    if "overlay_map" in st.session_state and frame_index in st.session_state["overlay_map"]:
        preview_img = st.session_state["overlay_map"][frame_index]
    else:
        preview_img = frame_np.copy()

    points = st.session_state.get("points", {}).get(frame_index, [])
    for x, y, l in points:
        color = (0, 255, 0) if l == 1 else (0, 0, 255)
        cv2.circle(preview_img, (int(x), int(y)), 5, color, -1)

    click_mode = st.radio("ç‚¹å‡»æ¨¡å¼", ["ä¿ç•™ç‚¹", "å»é™¤ç‚¹"], index=0, horizontal=True)
    labelP = 1 if click_mode == "ä¿ç•™ç‚¹" else 0

    # å·¦å›¾å³æ§
    col1, col2 = st.columns([3, 1])

    with col1:
        click = streamlit_image_coordinates(preview_img, key=f"frame_{frame_index}")

    with col2:
        st.markdown("### ğŸï¸ å½“å‰å¸§æ§åˆ¶")
        frame_index = st.slider("å¸§ä½ç½®", 0, len(frame_files) - 1, value=frame_index, key="frame_index")
        st.write(f"å½“å‰å¸§ç¼–å·ï¼š**{frame_index}**")

        # æ ‡ç­¾ç®¡ç†
        st.subheader("ğŸ·ï¸ æ ‡ç­¾è¾“å…¥ä¸ç¡®è®¤")
        if "label_history" not in st.session_state:
            st.session_state["label_history"] = []
        label_input = st.text_input("âœï¸ è¾“å…¥æ ‡ç­¾å", value="", placeholder="e.g. tomato")
        if label_input:
            suggestions = [l for l in st.session_state["label_history"] if l.startswith(label_input.lower())]
            if suggestions:
                st.markdown("ğŸ” è‡ªåŠ¨è¡¥å…¨å»ºè®®ï¼š" + ", ".join(suggestions[:5]))

        if st.button("âœ… ç¡®å®šæ ‡ç­¾"):
            label = label_input.strip().lower()
            if label:
                if label not in st.session_state["label_history"]:
                    st.session_state["label_history"].append(label)
                st.session_state["current_label"] = label
                st.success(f"âœ… å½“å‰ä½¿ç”¨æ ‡ç­¾ï¼š`{label}`")
            else:
                st.warning("âš ï¸ æ ‡ç­¾ä¸èƒ½ä¸ºç©º")

        label = st.session_state.get("current_label", None)

        if click:
            if "points" not in st.session_state:
                st.session_state["points"] = {}
            if frame_index not in st.session_state["points"]:
                st.session_state["points"][frame_index] = []
            st.session_state["points"][frame_index].append((click["x"], click["y"], labelP))
            st.rerun()

        if st.button("ğŸ§¹ æ¸…é™¤å½“å‰å¸§æ‰€æœ‰ç‚¹"):
            if "points" in st.session_state and frame_index in st.session_state["points"]:
                st.session_state["points"][frame_index] = []
                if "overlay_map" in st.session_state and frame_index in st.session_state["overlay_map"]:
                    del st.session_state["overlay_map"][frame_index]
                st.success("âœ… å½“å‰å¸§ç‚¹æ¸…é™¤å®Œæ¯•")
                st.experimental_rerun()

        if st.button("ğŸ§¼ æ¸…é™¤æ‰€æœ‰å¸§çš„ç‚¹"):
            st.session_state["points"] = {}
            if "overlay_map" in st.session_state:
                st.session_state["overlay_map"] = {}
            st.success("âœ… æ‰€æœ‰å¸§ç‚¹æ¸…é™¤å®Œæ¯•")
            st.experimental_rerun()

        if st.button("ğŸ‘ï¸ é¢„è§ˆå½“å‰å¸§æ ‡æ³¨"):
            if not points:
                st.warning("âš ï¸ å½“å‰å¸§æ— ç‚¹å‡»ç‚¹")
            else:
                pts = []
                lbls = []
                for x, y, l in points:
                    pts.append([x, y])
                    lbls.append(l)
                with torch.autocast("cuda" if torch.cuda.is_available() else "cpu"):
                    inference_state = sam2_model.init_state(segment_path)
                    frame_idx, obj_ids, masks = sam2_model.add_new_points_or_box(
                        inference_state=inference_state,
                        frame_idx=frame_index,
                        obj_id=0,
                        points=pts,
                        labels=lbls,
                        clear_old_points=True,
                        normalize_coords=False
                    )
                    mask_tensor = (masks[0] > 0.0)
                    if mask_tensor.any():
                        box = masks_to_boxes(mask_tensor)[0].int().cpu().tolist()
                    else:
                        st.warning("âš ï¸ ç”Ÿæˆçš„æ©ç ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—å¤–æ¥æ¡†")
                        box = [0, 0, 0, 0]
                    mask = mask_tensor[0].byte().cpu().numpy()
                    x1, y1, x2, y2 = box
                    overlay = frame_np.copy()
                    overlay[mask == 1] = (overlay[mask == 1] * 0.5 + np.array([128, 128, 255]) * 0.5).astype(np.uint8)
                    for x, y, l in points:
                        cv2.circle(overlay, (int(x), int(y)), 5, (0,255,0) if l==1 else (0,0,255), -1)
                    cv2.rectangle(overlay, (x1, y1), (x2, y2), (0,255,0), 2)
                    st.image(overlay, caption=f"æ ‡ç­¾: {label} | ç‚¹æ•°: {len(points)} | BBox: {box}")
                    if label is not None and label in st.session_state["label_history"]:
                        label_id = st.session_state["label_history"].index(label)
                        save_dir = f"yolo_labels_{session_id}"
                        os.makedirs(save_dir, exist_ok=True)
                        h, w = mask.shape
                        yolo_line = f"{label_id} {(x1+x2)/2/w:.6f} {(y1+y2)/2/h:.6f} {(x2-x1)/w:.6f} {(y2-y1)/h:.6f}\n"
                        label_file = os.path.join(save_dir, frame_files[frame_index].replace(".jpg", ".txt"))
                        with open(label_file, "w") as f:
                            f.write(yolo_line)
                        st.success(f"âœ… å•å¸§æ ‡ç­¾ä¿å­˜æˆåŠŸ: {label_file}")

        if st.button("âš¡ è‡ªåŠ¨æ©ç ä¼ æ’­"):
            ref_points = st.session_state["points"].get(frame_index, [])
            if not ref_points or not label:
                st.warning("âš ï¸ é¦–å¸§æœªæ‰“ç‚¹æˆ–æ ‡ç­¾æœªè®¾ç½®")
            else:
                save_dir = f"yolo_labels_{session_id}"
                os.makedirs(save_dir, exist_ok=True)
                label_id = st.session_state["label_history"].index(label)
                st.session_state["overlay_map"] = {}
                # æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹© CUDA æˆ– CPU
                with torch.autocast("cuda" if torch.cuda.is_available() else "cpu"):
                    inference_state = sam2_model.init_state(segment_path)
                    pts = [[p[0], p[1]] for p in ref_points]
                    lbls = [p[2] for p in ref_points]
                    sam2_model.add_new_points_or_box(
                        inference_state=inference_state,
                        frame_idx=frame_index,
                        obj_id=0,
                        points=pts,
                        labels=lbls,
                        clear_old_points=True,
                        normalize_coords=False
                    )
                    # æ‰¹é‡ä¼ æ’­
                    video_segments = {}
                    for out_frame_idx, out_obj_ids, out_mask_logits in sam2_model.propagate_in_video(inference_state):
                        video_segments[out_frame_idx] = {
                            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()
                            for i, out_obj_id in enumerate(out_obj_ids)
                        }
                    # éå†æ¯å¸§ï¼Œä¿å­˜YOLOæ ‡ç­¾å’Œé¢„è§ˆ
                    for i, frame_file in enumerate(frame_files):
                        mask = video_segments.get(i, {}).get(0, None)
                        if mask is None:
                            continue
                        device = 'cuda' if torch.cuda.is_available() else 'cpu'
                        mask_tensor = torch.from_numpy(mask[None]).to(device)
                        if mask_tensor.any():
                            box = masks_to_boxes(mask_tensor)[0].int().cpu().tolist()
                        else:
                            st.warning(f"âš ï¸ ç¬¬{i}å¸§æ©ç ä¸ºç©ºï¼Œè·³è¿‡å¤–æ¥æ¡†è®¡ç®—")
                            continue
                        x1, y1, x2, y2 = box
                        h, w = mask.shape
                        yolo_line = f"{label_id} {(x1+x2)/2/w:.6f} {(y1+y2)/2/h:.6f} {(x2-x1)/w:.6f} {(y2-y1)/h:.6f}\n"
                        label_file = os.path.join(save_dir, frame_file.replace(".jpg", ".txt"))
                        with open(label_file, "w") as f:
                            f.write(yolo_line)
                        img_path = os.path.join(FRAME_DIR, frame_file)
                        img = np.array(Image.open(img_path).convert("RGB"))
                        overlay = img.copy()
                        overlay[mask == 1] = (overlay[mask == 1] * 0.5 + np.array([128,128,255]) * 0.5).astype(np.uint8)
                        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0,255,0), 2)
                        st.session_state["overlay_map"][i] = overlay
                st.success("âœ… æ‰€æœ‰å¸§æ ‡ç­¾ä¸å›¾åƒå·²è‡ªåŠ¨ç”Ÿæˆï¼Œå¯é€å¸§é¢„è§ˆ") 